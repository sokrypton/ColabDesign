{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sokrypton/ColabDesign/blob/main/af/examples/use_esm_1b_bias.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5LVw2BwijkB"
      },
      "source": [
        "#ESM_1b\n",
        "use logits from ESM_1b as prior to AfDesign"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8j12AU-SikvN"
      },
      "outputs": [],
      "source": [
        "!pip -q install fair-esm\n",
        "import esm\n",
        "import gc\n",
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63LNj3dHilRW"
      },
      "outputs": [],
      "source": [
        "model, alphabet = esm.pretrained.esm1b_t33_650M_UR50S()\n",
        "\n",
        "# run model on GPU if available\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "model = model.eval()\n",
        "model.args.token_dropout = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ql8K460bjQtv"
      },
      "outputs": [],
      "source": [
        "def get_bias_from_esm(seq, p=None):\n",
        "  '''p=None; number of calculation done in parallel (increase if you have more gpu-memory)'''\n",
        "\n",
        "  # map esm-alphabet to standard-alphabet\n",
        "  tmp_a2n = {a:n for n,a in enumerate(alphabet.all_toks[4:24])}\n",
        "  tmp_aa_map = np.array([tmp_a2n[a] for a in \"ARNDCQEGHILKMFPSTWYV\"])\n",
        "\n",
        "  x,ln = alphabet.get_batch_converter()([(None,seq)])[-1],len(seq)\n",
        "  if p is None: p = ln\n",
        "  with torch.no_grad():\n",
        "    f = lambda x: model(x)[\"logits\"][:,1:(ln+1),4:24]\n",
        "    logits = np.zeros((ln,20))\n",
        "    for n in range(0,ln,p):\n",
        "      m = min(n+p,ln)\n",
        "      x_h = torch.tile(torch.clone(x),[m-n,1])\n",
        "      for i in range(m-n):\n",
        "        x_h[i,n+i+1] = alphabet.mask_idx\n",
        "      fx_h = f(x_h.to(device))\n",
        "      for i in range(m-n):\n",
        "        logits[n+i] = fx_h[i,n+i].cpu().numpy()\n",
        "  \n",
        "    return logits[:,tmp_aa_map]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrD1PRRMkZt3"
      },
      "outputs": [],
      "source": [
        "# get bias\n",
        "seq = \"MSTIEERVKKIIGEQLGVKQEEVTNNASFVEDLGADSLDTVELVMALEEEFDTEIPDEEAEKITTVQAAIDYINGHQA\"\n",
        "bias = get_bias_from_esm(seq)\n",
        "np.savetxt(\"bias.txt\",bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_8S9rXOkwZ8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(bias.T)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clear GPU memory\n",
        "del model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "10_kZfNQJpsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA2k3sAYuiXe"
      },
      "source": [
        "#AfDesign \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-AXy0s_4cKaK"
      },
      "outputs": [],
      "source": [
        "#@title install\n",
        "%%bash\n",
        "if [ ! -d params ]; then\n",
        "  pip -q install git+https://github.com/sokrypton/ColabDesign.git\n",
        "  mkdir params\n",
        "  curl -fsSL https://storage.googleapis.com/alphafold/alphafold_params_2022-03-02.tar | tar x -C params\n",
        "  for W in openfold_model_ptm_1 openfold_model_ptm_2 openfold_model_no_templ_ptm_1\n",
        "  do wget -qnc https://files.ipd.uw.edu/krypton/openfold/${W}.npz -P params; done\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vt7G_nbNeSQ3"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "import os\n",
        "from colabdesign import mk_afdesign_model, clear_mem\n",
        "from IPython.display import HTML\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "from alphafold.common import residue_constants\n",
        "\n",
        "def get_pdb(pdb_code=\"\",alphafold_model=False):\n",
        "  if pdb_code is None or pdb_code == \"\":\n",
        "    upload_dict = files.upload()\n",
        "    pdb_string = upload_dict[list(upload_dict.keys())[0]]\n",
        "    with open(\"tmp.pdb\",\"wb\") as out: out.write(pdb_string)\n",
        "    return \"tmp.pdb\"\n",
        "  else:\n",
        "    if alphafold_model:\n",
        "      os.system(f\"wget -qnc https://alphafold.ebi.ac.uk/files/AF-{pdb_code}-F1-model_v2.pdb\")\n",
        "      return f\"AF-{pdb_code}-F1-model_v2.pdb\"      \n",
        "    else:\n",
        "      os.system(f\"wget -qnc https://files.rcsb.org/view/{pdb_code}.pdb\")\n",
        "      return f\"{pdb_code}.pdb\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLd1DsnKzxBJ"
      },
      "outputs": [],
      "source": [
        "clear_mem()\n",
        "model = mk_afdesign_model(protocol=\"fixbb\",\n",
        "                        use_templates=False) # set True to constrain structure\n",
        "\n",
        "pdb_filename = get_pdb(\"P0A6A8\",alphafold_model=True)\n",
        "model.prep_inputs(pdb_filename, chain=\"A\")\n",
        "\n",
        "print(\"length\",  model._len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZGyGfMTmdXE"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "bias = np.loadtxt(\"bias.txt\")\n",
        "plt.imshow(bias.T,cmap=\"bwr_r\",vmin=-10,vmax=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeVuwTYWmQmp"
      },
      "outputs": [],
      "source": [
        "model.restart()\n",
        "model.opt[\"bias\"] = bias\n",
        "\n",
        "model.design_3stage(50,50,10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cW1KQiHKJpfp"
      },
      "outputs": [],
      "source": [
        "HTML(model.animate())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDrChASGVUUx"
      },
      "outputs": [],
      "source": [
        "model.get_seqs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEApO8YzBoS0"
      },
      "outputs": [],
      "source": [
        "model.save_pdb(f\"{model.protocol}.pdb\")\n",
        "model.plot_pdb()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0Xd7zQ-PqTly"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "q4qiU9I0QHSz"
      ],
      "name": "use_esm_1b_bias.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
